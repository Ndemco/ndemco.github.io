---
layout: post
title: "CI/CDogma: When Continuous Deployment Becomes Performative Engineering"
---

Somewhere along the way, continuous deployment stopped being an engineering decision and started being a personality trait.

Teams brag about deployment frequency like it’s a proxy for competence. Companies announce they’re “moving to continuous deployment” with the same energy they announce their next half-baked AI feature.

This is backwards.

Continuous deployment is neither good nor bad. It’s not a goal, a maturity badge, or proof that your engineering org has “made it.” It’s a solution to a very specific set of problems that arise under very specific conditions. Treating it as a universal best practice is how you end up with fast pipelines delivering bad software more efficiently than ever.

---
## Cargo Cult Engineering, but Automated

Continuous deployment has been enthusiastically cargo-culted.

Big tech companies do continuous deployment. Big tech companies are successful. Therefore, continuous deployment must be the reason they’re successful. So let's copy it.

This logic is flawed for more than one reason. For one, not every big tech company embraces continuous deployment; especially for high-risk, high-blast-radius systems.

But even setting that aside, what gets missed—every single time—is that continuous deployment works because those companies have built an environment where it *can* work. Not just robust pipelines with automated testing. The pipelines are obvious. What’s not as obvious is the hiring bar, the culture, and the expectations. That’s where most of the heavy lifting happens.

You cannot copy outcomes by copying rituals.

---

## Continuous Deployment Is Power, Not Just Automation

Continuous deployment, in its purest form, gives developers an enormous amount of power. Push a commit, run the pipeline, and congratulations—you’re in production.

That’s not just automation. It's delegation of responsibility.

When that power is in the hands of engineers who understand systems, failure modes, and long-term maintenance, it’s incredible. When it’s not, continuous deployment doesn’t improve quality or speed—it just accelerates damage.

The difference is not YAML. It’s people.

---

## The Triangle Everyone Tries to Cheat

Every tech company wants three things:

1. **High quality of delivery**
2. **High quantity of delivery**
3. **Inexpensive engineers**

You can pick two. You cannot have all three, no matter how optimistic your roadmap or how fancy your pipeline UI looks.

Continuous deployment is exceptional at maximizing *quantity*. Faster merges. Faster releases. Shorter feedback loops. That part works exactly as advertised. But once you crank up delivery speed, you have to decide which of the other two you’re willing to give up.

This is where most companies quietly make the worst possible choice.

---

## How Continuous Deployment Goes Off the Rails

The most common continuous deployment setup I see looks something like this:

- Engineers can deploy straight to production
- Code reviews are shallow or rushed (largely performative)
- Manual QA is gone, replaced by “the pipeline”
- The team is staffed with engineers who struggle to produce high-quality systems, regardless of tenure.

What you end up with is not high-performance engineering. It’s high-throughput mediocrity. Code gets shipped constantly, but it’s buggy, slow, brittle, and increasingly unmaintainable. You’re technically “moving fast,” but only because you’re dragging a growing pile of problems behind you.

That’s not modern engineering. That’s skipping steps and hoping automation compensates for bad judgment. Spoiler alert: it doesn't.

---

## Why Big Tech Gets Away With It

Companies like Netflix, Google, and Amazon can thrive in continuous deployment environments in large part because they invest heavily in engineering talent.

They hire people who can safely operate with direct access to production. People who understand systems deeply, think in failure modes, and take ownership seriously. That level of competence is expensive, and those companies are willing to pay for it.

That doesn’t mean continuous deployment only works if you match big tech compensation dollar-for-dollar. There are plenty of exceptional engineers who have no interest in big tech. They'd rather be a big fish in a small pond. They value autonomy, ownership, and work-life balance more than prestige.

What they will not do is accept substandard pay.

Even engineers who choose smaller companies still expect to be paid well. Not extravagantly. Not irrationally. But commensurate with their skill level and the level of trust and responsibility you’re asking of them. Continuous deployment fundamentally asks engineers to take on production risk. If you want people who can handle that responsibility, you have to meet the market where it actually is.

This isn’t about luxury salaries. It’s about respecting the cost of competence. If you want above average engineers, you need to offer above average pay.

---

## Continuous Deployment Thrives on Specialization, Not Just Talent

There’s another critical reason continuous deployment works so well at high-performing companies that almost never gets discussed: **they embrace specialization and domain ownership.**

Continuous deployment assumes domain mastery. It’s a bet on **deep familiarity**, not just general competence.

It works when engineers:
- Own specific services
- Live in the same codebases long enough to internalize them
- Are accountable for long-term outcomes, not just short-term metrics

It fails when engineers are treated as interchangeable resources, shuffled between repos, optimizing for throughput instead of understanding.

Without specialization and ownership, continuous deployment doesn’t amplify excellence. It amplifies risk.

Consider an engineer named John.

John is part of a team that owns four services, and for the last five years, most of his day-to-day work has lived inside those same four codebases.

He knows them intimately—the strengths, the weaknesses, the tech debt, the failure modes. You can’t run `git blame` on those repositories without seeing John's name everywhere.

When John gets a feature request, he already knows how he’s going to implement it before he opens an editor. Not because he’s psychic, but because he understands the system deeply. He understands the domain.

Give John the power to push directly to production, and he thrives. Continuous deployment works beautifully here—not because of the pipeline, but because of the expertise behind it.

Now consider Sam.

Sam works in an organization where teams don’t own services—they own KPIs. Any developer could be contributing to any repo, at any given time. Whatever changes are required for your team's KPIs, are your responsibility.

To hit this quarter’s KPI, Sam needs to add a feature to a repository he’s never touched before, in a domain he doesn’t understand, for a system no one truly owns. He’s expected to move fast, because velocity is the metric that matters.

Sam is now pushing code directly to production, in unfamiliar territory, with no real sense of the blast radius.

How do we think that’s going to go?

This is not an argument for gatekeeping. Ownership doesn’t mean exclusivity. It means the team that owns a system should be doing most of the work on it. Not because others are forbidden, but because that’s where the context lives. That’s where changes are cheapest, judgment is strongest, and risk is best understood.

---

## But the DORA Metrics!

I know at this point someone must be ready to wave the DORA metrics around like a trump card.

High-performing teams deploy more often. They have shorter lead times. They recover faster. Therefore, the thinking goes, deploying more often must *cause* high performance. So if we just crank up deployment frequency, we’ll magically become a high-performing team too.

This is a classic chicken-and-egg mistake.

The more reasonable conclusion to draw from the DORA metrics is not that frequent deployments *create* high-performing teams, but that **high-performing teams are capable of deploying frequently**.

Those teams tend to be staffed with strong engineers. They tend to have good ownership models. They tend to understand their systems deeply. In other words, they have already earned the right to move fast.

Frequent deployment is a *symptom* of those conditions, not the cause.

Treating deployment frequency as the lever instead of the outcome is how organizations end up copying the behavior without building the capability. You get the pipeline, the dashboards, and the metrics — but not the underlying competence that made those metrics meaningful in the first place.

---

## The Problem Isn’t Continuous Deployment. It’s Performative Continuous Deployment.

Continuous deployment is not broken. It’s also not mandatory.

There is nothing wrong with slower, more deliberate workflows. Manual QA. Staged releases. Longer review cycles. Explicit release windows. For many teams, these produce better software with fewer surprises. Stability is a feature. Predictability is a feature. Shipping less often is not inherently bad.

The mistake is adopting continuous deployment because it *feels* modern, not because it actually fits your organization.

---

## If You’re Going to Do Continuous Deployment, Do It on Purpose

If continuous deployment genuinely solves your problems, great. Commit to it intentionally. Invest in engineers who can handle production responsibility and create a culture where ownership matters more than process.

But if you’re doing continuous deployment just so you can say you do continuous deployment, stop.

You’re not behind. You’re not old-fashioned. You’re not failing engineering culture. You’re just choosing a workflow that matches your reality instead of cosplaying a company with a completely different one.

Continuous Deployment is a tool. Continuous Dogma is what happens when you stop thinking.
